Alright, story time.

I've built this thing so I could roughly benchmark throughput of my IFTTT service.

I figured that before I start testing the complex interconnected service, I should mock it in the most simplest of terms. In this case, it's an express webserver that makes a connection to remote service in response to incoming connection.

After some fiddling, I came up with a number that this mock server can handle about 800RPS per core without introducing too much noise into the picture. After that, the response time starts to fluctuate pretty rapidly which is most likely caused by event loop delays due to GC.

I also found a hard limit of about 2500RPS no matter whether I'm running 3 or 4 mock servers in parallel. Given that my laptop has only 4 cores and 8 threads 4 of which are taken by load generators (who are also web servers that suppose to handle webhooks in 1:1 ratio to requests sent), I'd say it's probably not all that surprising.

Running benchmark in swarm mode also confirmed that no matter the ratio between mock servers and load generators, the throughput peaks at about 2500RPS on my machine. In fact, increasing number of load generators reduces the overall throughput of the system, presumably, because of the ambient cpu load each of them produces. If this assumption is correct, this would likely mean that we're indeed mostly bound by cpu utilization rather than some underlying system level limit on file descriptors and such.

Somewhat predictably, but still worth checking, removing the whole webhook part of the routine increases throughput, wait for it, twofold.

As a side note, I've tried using Caddy1 to load balance between mock servers and quickly discovered that caddy itself has pretty low concurrency ceiling out the box. Docker's dnsrr, unsurprisingly enough, seem to be doing much better.

In general, I'd say that the initial goal is reached and I'm ready to start testing IFTTT service with that. I do not expect it to be even remotely close to 800RPS throughput per core.

As for this project, I think there's still some room for improvement both is throughput and in presentation of results.

I've implemented some simple controller for load generators based on websockets. It seems there's a noticeable impact on the throughput of the system due to increases in CPU utilization. Still, it seem to be worth the price if we are to move to cloud with this tool as managing a heap of json files across multiple machines is bound to be a pain. So far I'm seeing about 25% decrease in RPS, but this number may change both ways both due me finding bottlenecks and due to adding more metrics to calculate.

I've added few more metrics such as rtt and e2e (time between sending a request and receiving the webhook) and a UI that allows me to see results in near real time.

In the process of building the UI, the feeling that I'm inventing a wheel suddenly intensified so I decided to take a detour and see what kind of results I can get with prometheus, ab and a webhook sink in form of nginx.

I've observed that a single instance of `ab` sending requests to a single instance of `mockserver` stabilizes at about 7000 RPS and 18ms eventloop lag with concurrency of 1000. Further increasing concurrency only increases eventloop lag without affecting RPS. Increasing concurrency beyond some level, however, drops RPS significantly. Increasing number of `ab` replicas does not raise RPS either, it only increases likelihood of `ab` reaching timeout before it makes all the requests.

Another benefit of using this approach is that I now have an access to the trove of metrics prometheus provides so it's easier for me spot bottlenecks (and makes it possible to spot them retrospectively).

What I don't like in current setup is that `ab` is pretty crude tool and does not allow me to fine tune the load to the degree I want to. I'm now inclined to try some other modern tools.

K6 seems to be a decent tool to make sure your app continually meets your performance expectations. I think that next thing for me to do would be to integrate it into testing pipeline for IFTTT project along with prometheus and nginx-based sink for webhooks and see the numbers I'm getting. I went way overboard with this benchmark project and while it was a cool learning experience, I think it's time to start applying this knowledge to a real use case.

Further research may include this TODO items:
- Throughput might be limited by something inbetween mock server and load generator be it dnsrr, limit on file descriptors or anything else.

- In general it makes sense to start moving from my laptop to a cloud platform that is better suited for that kind of testing and provides more space for scalability. I bet Terraform would be of great help there.

- Prometheus default nodejs exported provides a trove of metrics and you can add quite few on top of them, but for now I'm clueless on the effect load makes on a lot of them. There's a wide field for additioal research on the matter.
